# src/tools/youtube_tools.py

from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain_community.document_loaders import YoutubeLoader
from langchain_community.tools import YouTubeSearchTool

# Importamos la definici√≥n de AgentState desde el archivo agent.py
# El '..' indica que subimos un nivel en la estructura de directorios para encontrar el m√≥dulo.
from typing import TypedDict, List
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    topic: str
    video_urls: List[str]
    video_metadata: List[dict]
    summaries: List[str]
    report: str
    messages: List[BaseMessage]

# --------------------------------------------------------------------------
# NODO 1: B√öSQUEDA DE V√çDEOS EN YOUTUBE
# --------------------------------------------------------------------------
def search_videos_node(state: AgentState) -> dict:
    """
    Busca v√≠deos en YouTube basados en el tema proporcionado en el estado.

    Este nodo utiliza la herramienta 'YouTubeSearchTool' de LangChain para
    encontrar las 10 URLs de v√≠deo m√°s relevantes para el tema de investigaci√≥n.

    Args:
        state (AgentState): El estado actual del agente, que debe contener el 'topic'.

    Returns:
        dict: Un diccionario con la clave 'video_urls' para actualizar el estado del agente.
    """
    print("\n--- üîé NODO: BUSCANDO V√çDEOS ---")
    topic = state["topic"]
    print(f"Tema de b√∫squeda: {topic}")

    try:
        # Inicializamos la herramienta de b√∫squeda de YouTube.
        tool = YouTubeSearchTool()

        # Ejecutamos la b√∫squeda con solo el tema
        search_results_str = tool.run(topic)

        # Convertimos la cadena de resultados en una lista real de Python.
        video_urls = eval(search_results_str)

        print(f"‚úÖ Se encontraron {len(video_urls)} v√≠deos.")
        return {"video_urls": video_urls}

    except Exception as e:
        print(f"‚ùå Error durante la b√∫squeda de v√≠deos: {e}")
        # Si hay un error, devolvemos una lista vac√≠a para no detener el flujo.
        return {"video_urls": []}


# --------------------------------------------------------------------------
# NODO 2: EXTRACCI√ìN Y RESUMEN DE TRANSCRIPCIONES
# --------------------------------------------------------------------------
def summarize_videos_node(state: AgentState) -> dict:
    """
    Para cada URL de v√≠deo, extrae su transcripci√≥n y genera un resumen ejecutivo.

    Este nodo itera sobre las 'video_urls' del estado. Para cada una, utiliza
    'YoutubeLoader' para obtener la transcripci√≥n y luego un LLM con una cadena
    de resumen para crear un resumen t√©cnico.

    Args:
        state (AgentState): El estado actual del agente, que contiene 'video_urls'.

    Returns:
        dict: Un diccionario con 'summaries' y 'video_metadata' para actualizar el estado.
    """
    print("\n--- üìù NODO: EXTRAYENDO Y RESUMIENDO V√çDEOS ---")
    video_urls = state["video_urls"]
    summaries = []
    video_metadata = []

    if not video_urls:
        print("‚ö†Ô∏è No se encontraron v√≠deos para resumir. Saltando este paso.")
        return {"summaries": [], "video_metadata": []}

    # Inicializamos el modelo de lenguaje que usaremos para resumir.
    # 'gpt-3.5-turbo-16k' es una buena opci√≥n por su gran ventana de contexto.
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k")

    # Cargamos una "cadena de resumen" de LangChain.
    # 'map_reduce' es eficiente para documentos largos como las transcripciones.
    summarize_chain = load_summarize_chain(
        llm,
        chain_type="map_reduce"
    )

    for i, url in enumerate(video_urls):
        # Si la URL ya es completa, la usamos tal como est√°
        if url.startswith('https://'):
            full_url = url
        else:
            # Si es solo un sufijo, agregamos el dominio
            full_url = f"https://www.youtube.com{url}"
        
        # Limpiar caracteres HTML codificados y extraer solo el ID del video
        full_url = full_url.replace('&amp;', '&')
        
        # Extraer solo el ID del video para crear una URL limpia
        if 'watch?v=' in full_url:
            video_id = full_url.split('watch?v=')[1].split('&')[0]
            full_url = f"https://www.youtube.com/watch?v={video_id}"
        
        print(f"\nProcesando v√≠deo {i+1}/{len(video_urls)}: {full_url}")

        try:
            # Usamos el cargador de YouTube de LangChain.
            loader = YoutubeLoader.from_youtube_url(full_url, add_video_info=True)
            docs = loader.load()

            # Extraemos los metadatos antes de resumir
            metadata = docs[0].metadata
            title = metadata.get("title", "T√≠tulo no disponible")
            author = metadata.get("author", "Autor no disponible")
            print(f"  - T√≠tulo: {title}")

            # Ejecutamos la cadena de resumen sobre la transcripci√≥n.
            summary = summarize_chain.run(docs)
            summaries.append(summary)

            video_metadata.append({
                "title": title,
                "author": author,
                "url": full_url
            })
            print("  - ‚úÖ Resumen generado.")

        except Exception as e:
            print(f"  - ‚ö†Ô∏è No se pudo procesar el v√≠deo {full_url}: {e}")
            # Si hay un error (ej. sin transcripci√≥n), a√±adimos un marcador
            # para que el informe final refleje que este v√≠deo no se pudo procesar.
            summaries.append("No fue posible generar un resumen para este v√≠deo (puede que no tenga transcripci√≥n).")
            video_metadata.append({
                "title": f"V√≠deo no procesado en {full_url}",
                "author": "Desconocido",
                "url": full_url
            })

    return {"summaries": summaries, "video_metadata": video_metadata}